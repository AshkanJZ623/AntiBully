{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olPP3VpM6oqw"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire easyocr pillow transformers==4.19.2 keras-ocr kaggle pytorch_lightning\n",
        "!pip install python-magic"
      ],
      "metadata": {
        "id": "mC2YWhNXlUpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d2db14-0e41-4674-899e-ea9acf77c49d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting transformers==4.19.2\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-ocr\n",
            "  Downloading keras_ocr-0.9.3-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.2)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.2) (4.66.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.16.0+cu118)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.8.1.78)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.2)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.6.2)\n",
            "Collecting efficientnet==1.0.0 (from keras-ocr)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting essential_generators (from keras-ocr)\n",
            "  Downloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fonttools in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (4.45.1)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.4.0)\n",
            "Collecting validators (from keras-ocr)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet==1.0.0->keras-ocr)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.5.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.2) (3.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (3.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (3.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=f417a620e7f634909dbb60cd39ad11e3d8413e5105a65b8079a9decfb1aba92b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: tokenizers, pyclipper, ninja, essential_generators, validators, python-bidi, lightning-utilities, fire, keras-applications, transformers, torchmetrics, efficientnet, pytorch_lightning, keras-ocr, easyocr\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed easyocr-1.7.1 efficientnet-1.0.0 essential_generators-1.0 fire-0.5.0 keras-applications-1.0.8 keras-ocr-0.9.3 lightning-utilities-0.10.0 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.4.2 pytorch_lightning-2.1.2 tokenizers-0.12.1 torchmetrics-1.2.1 transformers-4.19.2 validators-0.22.0\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: python-magic\n",
            "Successfully installed python-magic-0.4.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "!wget https://raw.githubusercontent.com/oyyd/frozen_east_text_detection.pb/master/frozen_east_text_detection.pb\n",
        "!wget https://github.com/miccunifi/ISSUES/releases/download/latest/hmc_text-inv-comb_best.ckpt\n",
        "!wget https://github.com/miccunifi/ISSUES/releases/download/latest/resources.zip\n",
        "!unzip resources.zip\n",
        "\n",
        "!gdown 1JhT8L7F-RnRQJTfifOndi_-TsukM-ErC # Download Image Model\n",
        "!gdown 1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX # Download COCO Weights\n",
        "!mv '/content/hmc_text-inv-comb_best.ckpt' '/content/resources/pretrained_models'"
      ],
      "metadata": {
        "id": "-cgN-mVpcbO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e78e782-87f6-4961-c694-914aac01aaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-97t3cy93\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-97t3cy93\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=ee66124c3d52fe0fc0c76bcb4e77d96477d865b0d7724f05b5b0e82e7dfb4a78\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-svtjsy9t/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-3oz2ilp8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-3oz2ilp8\n",
            "  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=5b6866bd23b426a450b9bc9dab84e5bbdbf4aa87fd42887077fec350a1c58827\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gbdqa330/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n",
            "--2023-12-04 05:21:37--  https://raw.githubusercontent.com/oyyd/frozen_east_text_detection.pb/master/frozen_east_text_detection.pb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96662756 (92M) [application/octet-stream]\n",
            "Saving to: ‘frozen_east_text_detection.pb’\n",
            "\n",
            "frozen_east_text_de 100%[===================>]  92.18M   430MB/s    in 0.2s    \n",
            "\n",
            "2023-12-04 05:21:44 (430 MB/s) - ‘frozen_east_text_detection.pb’ saved [96662756/96662756]\n",
            "\n",
            "--2023-12-04 05:21:44--  https://github.com/miccunifi/ISSUES/releases/download/latest/hmc_text-inv-comb_best.ckpt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/680551378/9cbd520a-1288-47ad-aec4-43f6ca9458ef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231204T052144Z&X-Amz-Expires=300&X-Amz-Signature=92dd6ca5557e6bee7a9f31ec1a2071b67e3c4149e192fac0dfc73d13c685a5c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=680551378&response-content-disposition=attachment%3B%20filename%3Dhmc_text-inv-comb_best.ckpt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-12-04 05:21:44--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/680551378/9cbd520a-1288-47ad-aec4-43f6ca9458ef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231204T052144Z&X-Amz-Expires=300&X-Amz-Signature=92dd6ca5557e6bee7a9f31ec1a2071b67e3c4149e192fac0dfc73d13c685a5c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=680551378&response-content-disposition=attachment%3B%20filename%3Dhmc_text-inv-comb_best.ckpt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1885201543 (1.8G) [application/octet-stream]\n",
            "Saving to: ‘hmc_text-inv-comb_best.ckpt’\n",
            "\n",
            "hmc_text-inv-comb_b 100%[===================>]   1.75G  13.2MB/s    in 2m 20s  \n",
            "\n",
            "2023-12-04 05:24:05 (12.8 MB/s) - ‘hmc_text-inv-comb_best.ckpt’ saved [1885201543/1885201543]\n",
            "\n",
            "--2023-12-04 05:24:05--  https://github.com/miccunifi/ISSUES/releases/download/latest/resources.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/680551378/1c813700-2f85-4f0f-a1eb-513a10f6ee5a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231204T052405Z&X-Amz-Expires=300&X-Amz-Signature=0bbf8dd764c36be4bc3c2e7395a26c442ccf0556e3d839916f52c2de02dc39fa&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=680551378&response-content-disposition=attachment%3B%20filename%3Dresources.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-12-04 05:24:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/680551378/1c813700-2f85-4f0f-a1eb-513a10f6ee5a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231204T052405Z&X-Amz-Expires=300&X-Amz-Signature=0bbf8dd764c36be4bc3c2e7395a26c442ccf0556e3d839916f52c2de02dc39fa&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=680551378&response-content-disposition=attachment%3B%20filename%3Dresources.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177722678 (169M) [application/octet-stream]\n",
            "Saving to: ‘resources.zip’\n",
            "\n",
            "resources.zip        59%[==========>         ] 100.80M  17.9MB/s    eta 5s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import whisper\n",
        "import clip\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "import IPython.display as display\n",
        "import skimage.io as io\n",
        "import random\n",
        "import pickle as pkl\n",
        "import json\n",
        "import easyocr\n",
        "import fire\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import keras_ocr\n",
        "import argparse\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "\n",
        "from transformers import ResNetForImageClassification\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from transformers import CLIPTokenizer, CLIPProcessor\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files, drive\n",
        "from IPython.display import Image\n",
        "from PIL import Image, ImageDraw\n",
        "from skimage import transform\n",
        "from skimage.feature import canny\n",
        "from skimage.color import rgb2gray, gray2rgb\n",
        "from multiprocessing import Pool\n",
        "from distutils.archive_util import make_archive\n",
        "from imutils.object_detection import non_max_suppression\n",
        "from clip.model import CLIP\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "FeMMTvjVu5tw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCHk27QY6yZe"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roKwLTvdVEpM"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrdbhSq6im8"
      },
      "source": [
        "# Load Text Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmpXYrA2UrPC"
      },
      "outputs": [],
      "source": [
        "text_model = AutoModelForSequenceClassification.from_pretrained(\"Kidsshield/CBD-FullData\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('Kidsshield/CBD-FullData')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "text_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MEME Text Model"
      ],
      "metadata": {
        "id": "DWy7gXSGpaHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meme_text_model = AutoModelForSequenceClassification.from_pretrained(\"Kidsshield/MEME\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('Kidsshield/MEME')\n",
        "\n",
        "meme_text_model.to(device)"
      ],
      "metadata": {
        "id": "zYkckMt0pYgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmoeKyCS6QSJ"
      },
      "source": [
        "# Load Image Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6f44bJUtdvG"
      },
      "outputs": [],
      "source": [
        "image_model = load_model('/content/seperatedclassesmodel.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM9wwlF-5q6p"
      },
      "source": [
        "# Text + Image Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r83EfWHqtcx3"
      },
      "outputs": [],
      "source": [
        "def perform_inference(input, meme=False):\n",
        "    if os.path.isfile(input):\n",
        "        return image_inference(input)\n",
        "    else:\n",
        "        if meme:\n",
        "            return text_inference(input)\n",
        "        else:\n",
        "            return text_inference(input)\n",
        "\n",
        "\n",
        "def text_inference(input):\n",
        "    encoding = tokenizer(input, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    prediction = 'Bully' if torch.argmax(output.logits).item() == 1 else 'NonBully'\n",
        "    print(\"Prediction:\", prediction)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def meme_text_inference(input):\n",
        "    encoding = tokenizer(input, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = meme_text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    prediction = 'Bully' if torch.argmax(output.logits).item() == 1 else 'NonBully'\n",
        "    print(\"Prediction:\", prediction)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def image_inference(input):\n",
        "\n",
        "    dic = {'drawing': 0, 'hentai': 1,'neutral' : 2,'porn':3,'sexy':4,'violence':5,'weapon':6}\n",
        "    icd = {v: k for k, v in dic.items()}\n",
        "    img = load_img(input, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    predictions = image_model.predict(img)\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "    predicted_label = icd[predicted_class[0]]\n",
        "    probability = round(np.max(predictions) * 100, 2)\n",
        "    print(\"Prediction:\", predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Audio model"
      ],
      "metadata": {
        "id": "EZVcWTplLN7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "WY02PmBnLNLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hO6vcSy9_KT"
      },
      "source": [
        "#MEME Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY9aZiRx5Z03"
      },
      "outputs": [],
      "source": [
        "# #set GPU id\n",
        "# CUDA_DEVICE=0\n",
        "# torch.cuda.set_device(CUDA_DEVICE)\n",
        "# device = torch.device(\"cuda:\"+str(CUDA_DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = os.path.join('/content/coco_weights.pt')"
      ],
      "metadata": {
        "id": "8gCRFQyKOKnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "D = torch.device"
      ],
      "metadata": {
        "id": "zj6Tzh-zOOUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "metadata": {
        "id": "iEw9qk0YOPBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ],
      "metadata": {
        "id": "EvOG39mLORw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device='cuda', jit=False)\n",
        "clip_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "NxefF2YdOdsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load model weights\n",
        "prefix_length = 10\n",
        "meme_model = ClipCaptionModel(prefix_length)\n",
        "meme_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "meme_model = meme_model.eval()\n",
        "meme_model = meme_model.to(device)"
      ],
      "metadata": {
        "id": "dIGoAVXnOhsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inpainting image\n",
        "\"\"\"\n",
        "src: https://pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/\n",
        "\"\"\"\n",
        "source_dir = '/content/'\n",
        "target_dir_masked = '/content/'\n",
        "target_dir_inpainted = '/content/'\n",
        "east_path = '/content/frozen_east_text_detection.pb'\n",
        "width = 320\n",
        "height = 320\n",
        "min_confidence = 0.5\n",
        "# img_fns = [x for x in os.listdir(source_dir) if x.endswith('.jpg') or x.endswith('.png') or x.endswith('.jpeg')]\n",
        "\n",
        "def transform_image(img_fp):\n",
        "\n",
        "    image = cv2.imread(img_fp)\n",
        "    masked, inpainted = image.copy(), image.copy()\n",
        "    (H, W) = image.shape[:2]\n",
        "    (newW, newH) = (width, height)\n",
        "    rW = W / float(newW)\n",
        "    rH = H / float(newH)\n",
        "    image = cv2.resize(image, (newW, newH))\n",
        "    (H, W) = image.shape[:2]\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
        "        (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    (scores, geometry) = net.forward(layerNames)\n",
        "\n",
        "    (numRows, numCols) = scores.shape[2:4]\n",
        "    rects = []\n",
        "    confidences = []\n",
        "\n",
        "    for y in range(0, numRows):\n",
        "\n",
        "        scoresData = scores[0, 0, y]\n",
        "        xData0 = geometry[0, 0, y]\n",
        "        xData1 = geometry[0, 1, y]\n",
        "        xData2 = geometry[0, 2, y]\n",
        "        xData3 = geometry[0, 3, y]\n",
        "        anglesData = geometry[0, 4, y]\n",
        "\n",
        "\n",
        "        for x in range(0, numCols):\n",
        "\n",
        "            if scoresData[x] < min_confidence:\n",
        "                continue\n",
        "\n",
        "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
        "            angle = anglesData[x]\n",
        "            cos = np.cos(angle)\n",
        "            sin = np.sin(angle)\n",
        "            h = xData0[x] + xData2[x]\n",
        "            w = xData1[x] + xData3[x]\n",
        "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
        "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
        "            startX = int(endX - w)\n",
        "            startY = int(endY - h)\n",
        "            rects.append((startX, startY, endX, endY))\n",
        "            confidences.append(scoresData[x])\n",
        "\n",
        "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
        "    mask_for_inpainting = np.zeros(inpainted.shape[:2], np.uint8)\n",
        "    for (startX, startY, endX, endY) in boxes:\n",
        "\n",
        "        startX = int(startX * rW)\n",
        "        startY = int(startY * rH)\n",
        "        endX = int(endX * rW)\n",
        "        endY = int(endY * rH)\n",
        "        cv2.rectangle(masked, (startX, startY), (endX, endY), (127, 127, 127), -1)\n",
        "        cv2.rectangle(mask_for_inpainting, (startX, startY), (endX, endY), 255, -1)\n",
        "\n",
        "    inpainted = cv2.inpaint(inpainted, mask_for_inpainting, 7, cv2.INPAINT_NS)\n",
        "\n",
        "    return masked, inpainted\n",
        "\n",
        "\n",
        "layerNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
        "print(\"[INFO] loading EAST text detector...\")\n",
        "net = cv2.dnn.readNet(east_path)\n",
        "\n",
        "def transform_and_save_image(img_fp):\n",
        "    path, img_fn = os.path.split(img_fp)\n",
        "    img_fp_inpainted = os.path.join(path, \"inpaint_\"+img_fn)\n",
        "\n",
        "    img_masked, img_inpainted = transform_image(img_fp)\n",
        "    return img_inpainted"
      ],
      "metadata": {
        "id": "JgXNcCMAT_QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference\n",
        "use_beam_search = True\n",
        "reader = easyocr.Reader(['en'])\n",
        "\n",
        "def process_meme_caption(file_path):\n",
        "    head, img_fn = os.path.split(file_path)\n",
        "    image = transform_and_save_image(file_path)\n",
        "    pil_image = PIL.Image.fromarray(image)\n",
        "\n",
        "\n",
        "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "        prefix_embed = meme_model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "    if use_beam_search:\n",
        "        generated_text_prefix = generate_beam(meme_model, clip_tokenizer, embed=prefix_embed)[0]\n",
        "    else:\n",
        "        generated_text_prefix = generate2(meme_model, clip_tokenizer, embed=prefix_embed)\n",
        "\n",
        "    return generated_text_prefix"
      ],
      "metadata": {
        "id": "Fu9k4MulOmXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISSUES MEME Inference"
      ],
      "metadata": {
        "id": "W1Gs9383xgfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras - OCR Module"
      ],
      "metadata": {
        "id": "neV9gXYVFL_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://shegocodes.medium.com/extract-text-from-image-left-to-right-and-top-to-bottom-with-keras-ocr-b56f098a6efe"
      ],
      "metadata": {
        "id": "5UOtcYUhFT-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distance(predictions):\n",
        "    \"\"\"\n",
        "    Function returns dictionary with (key,value):\n",
        "        * text : detected text in image\n",
        "        * center_x : center of bounding box (x)\n",
        "        * center_y : center of bounding box (y)\n",
        "        * distance_from_origin : hypotenuse\n",
        "        * distance_y : distance between y and origin (0,0)\n",
        "    \"\"\"\n",
        "\n",
        "    x0, y0 = 0, 0\n",
        "\n",
        "    detections = []\n",
        "    for group in predictions:\n",
        "\n",
        "        top_left_x, top_left_y = group[1][0]\n",
        "        bottom_right_x, bottom_right_y = group[1][1]\n",
        "        center_x, center_y = (top_left_x + bottom_right_x)/2, (top_left_y + bottom_right_y)/2\n",
        "\n",
        "        distance_from_origin = math.dist([x0,y0], [center_x, center_y])\n",
        "\n",
        "        distance_y = center_y - y0\n",
        "\n",
        "        detections.append({\n",
        "                            'text': group[0],\n",
        "                            'center_x': center_x,\n",
        "                            'center_y': center_y,\n",
        "                            'distance_from_origin': distance_from_origin,\n",
        "                            'distance_y': distance_y\n",
        "                        })\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n",
        "def distinguish_rows(lst, thresh=15):\n",
        "    \"\"\"Function to help distinguish unique rows\"\"\"\n",
        "    sublists = []\n",
        "    for i in range(0, len(lst)-1):\n",
        "        if (lst[i+1]['distance_y'] - lst[i]['distance_y'] <= thresh):\n",
        "            if lst[i] not in sublists:\n",
        "                sublists.append(lst[i])\n",
        "            sublists.append(lst[i+1])\n",
        "        else:\n",
        "            yield sublists\n",
        "            sublists = [lst[i+1]]\n",
        "    yield sublists"
      ],
      "metadata": {
        "id": "Whhr0f4dSkZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = keras_ocr.pipeline.Pipeline()"
      ],
      "metadata": {
        "id": "FWs3GSrsS3Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MyOCR(image_path):\n",
        "\n",
        "  read_image = keras_ocr.tools.read(image_path)\n",
        "  prediction_groups = pipeline.recognize([read_image])\n",
        "  predictions = prediction_groups[0]\n",
        "  predictions = get_distance(predictions)\n",
        "  predictions = list(distinguish_rows(predictions, thresh=15))\n",
        "  ordered_preds = []\n",
        "  for row in predictions:\n",
        "    row = sorted(row, key=lambda x:x['distance_from_origin'])\n",
        "    for each in row:\n",
        "      ordered_preds.append(each['text'])\n",
        "    last = ' '.join(ordered_preds)\n",
        "\n",
        "\n",
        "  return last"
      ],
      "metadata": {
        "id": "qvd3YuJNS3UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ISSUES INFERENCE"
      ],
      "metadata": {
        "id": "KgRld5mmFRi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Combiner(nn.Module):\n",
        "    def __init__(self, convex_tensor: bool, input_dim: int, comb_proj: bool, comb_fusion: str):\n",
        "        super(Combiner, self).__init__()\n",
        "        self.map_dim = input_dim\n",
        "        self.comb_proj = comb_proj\n",
        "        self.comb_fusion = comb_fusion\n",
        "        self.convex_tensor = convex_tensor\n",
        "\n",
        "        if self.convex_tensor:\n",
        "            branch_out_dim = self.map_dim\n",
        "        else:\n",
        "            branch_out_dim = 1\n",
        "\n",
        "        comb_in_dim = self.map_dim\n",
        "        comb_concat_out_dim = comb_in_dim\n",
        "\n",
        "        if self.comb_proj:\n",
        "            self.comb_image_proj = nn.Sequential(\n",
        "                nn.Linear(comb_in_dim, 2 * comb_in_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.5)\n",
        "            )\n",
        "\n",
        "            self.comb_text_proj = nn.Sequential(\n",
        "                nn.Linear(comb_in_dim, 2 * comb_in_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.5)\n",
        "            )\n",
        "\n",
        "            comb_in_dim = 2 * comb_in_dim\n",
        "\n",
        "        if self.comb_fusion == 'concat':\n",
        "            branch_in_dim = 2 * comb_in_dim\n",
        "        elif self.comb_fusion == 'align':\n",
        "            branch_in_dim = comb_in_dim\n",
        "        else:\n",
        "            ValueError()\n",
        "\n",
        "        self.comb_shared_branch = nn.Sequential(\n",
        "            nn.Linear(branch_in_dim, 2 * branch_in_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2 * branch_in_dim, branch_out_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.comb_concat_branch = nn.Sequential(\n",
        "            nn.Linear(branch_in_dim, 2 * branch_in_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2 * branch_in_dim, comb_concat_out_dim),\n",
        "        )\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super().__call__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, img_projection, post_features):\n",
        "        if self.comb_proj:\n",
        "            proj_img_fea = self.comb_image_proj(img_projection)\n",
        "            proj_txt_fea = self.comb_text_proj(post_features)\n",
        "        else:\n",
        "            proj_img_fea = img_projection\n",
        "            proj_txt_fea = post_features\n",
        "\n",
        "        if self.comb_fusion == 'concat':\n",
        "            comb_features = torch.cat([proj_img_fea, proj_txt_fea], dim=1)\n",
        "        elif self.comb_fusion == 'align':\n",
        "            comb_features = torch.mul(proj_img_fea, proj_txt_fea)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        side_branch = self.comb_shared_branch(comb_features)\n",
        "        central_branch = self.comb_concat_branch(comb_features)\n",
        "\n",
        "        features = central_branch + ((1 - side_branch) * img_projection + side_branch * post_features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "PHI_INPUT_DIM = 768\n",
        "\n",
        "\n",
        "class TextualInversion(nn.Module):\n",
        "    def __init__(self, clip_model: CLIP, clip_img_enc_output_dim: int, phi_proj: bool, text_proj: bool, post_proj: bool,\n",
        "                 drop_probs, phi_freeze: bool, enh_text: bool, post_dim=None, num_pre_proj_layers=1):\n",
        "        super(TextualInversion, self).__init__()\n",
        "\n",
        "        self.clip_model = clip_model\n",
        "        self.phi_proj = phi_proj\n",
        "        self.text_proj = text_proj\n",
        "        self.post_proj = post_proj\n",
        "        self.enh_text = enh_text\n",
        "\n",
        "        assert self.clip_model.token_embedding.embedding_dim == PHI_INPUT_DIM, 'CLIP model selected is not compatible' \\\n",
        "                                                                               ' with the pre-trained phi network'\n",
        "\n",
        "        if self.post_proj:\n",
        "            self.output_dim = post_dim\n",
        "        else:\n",
        "            self.output_dim = self.clip_model.token_embedding.embedding_dim\n",
        "\n",
        "        phi_layers = [nn.Linear(PHI_INPUT_DIM, 3072),\n",
        "                      nn.GELU(),\n",
        "                      nn.Dropout(p=0.5),\n",
        "                      nn.Linear(3072, 3072),\n",
        "                      nn.GELU(),\n",
        "                      nn.Dropout(p=0.5),\n",
        "                      nn.Linear(3072, PHI_INPUT_DIM),\n",
        "                      ]\n",
        "        self.phi = nn.Sequential(*phi_layers)\n",
        "\n",
        "        if phi_proj:\n",
        "\n",
        "            phi_map_layers = [nn.Linear(PHI_INPUT_DIM, PHI_INPUT_DIM),\n",
        "                              nn.Dropout(p=drop_probs[0])]\n",
        "            self.phi_map = nn.Sequential(*phi_map_layers)\n",
        "\n",
        "        phi_dict = torch.load(\"./resources/pretrained_weights/phi/phi_imagenet_45.pt\")[\"MLPCustom\"]\n",
        "        with torch.no_grad():\n",
        "            self.phi[0].weight.copy_(phi_dict['layers.0.weight'])\n",
        "            self.phi[0].bias.copy_(phi_dict['layers.0.bias'])\n",
        "            self.phi[3].weight.copy_(phi_dict['layers.3.weight'])\n",
        "            self.phi[3].bias.copy_(phi_dict['layers.3.bias'])\n",
        "            self.phi[6].weight.copy_(phi_dict['layers.6.weight'])\n",
        "            self.phi[6].bias.copy_(phi_dict['layers.6.bias'])\n",
        "\n",
        "        if phi_freeze:\n",
        "            for name, p in self.phi.named_parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "        in_dim = clip_img_enc_output_dim\n",
        "        pre_inversion_layers = [nn.Linear(in_dim, PHI_INPUT_DIM),\n",
        "                                nn.Dropout(p=drop_probs[0])]\n",
        "        for _ in range(1, num_pre_proj_layers):\n",
        "            pre_inversion_layers.extend(\n",
        "                [nn.ReLU(), nn.Linear(PHI_INPUT_DIM, PHI_INPUT_DIM), nn.Dropout(p=drop_probs[0])])\n",
        "        self.pre_inversion_map = nn.Sequential(*pre_inversion_layers)\n",
        "\n",
        "        if post_proj:\n",
        "\n",
        "            post_inversion_layers = [nn.Linear(self.clip_model.token_embedding.embedding_dim, post_dim),\n",
        "                                     nn.Dropout(p=drop_probs[0])]\n",
        "            self.post_inversion_map = nn.Sequential(*post_inversion_layers)\n",
        "\n",
        "    def encode_with_vstar(self, clip_model: CLIP, text: torch.tensor, v_star: torch.tensor, num_vstar=1,\n",
        "                          pooling=True, token_id=259, proj=True):\n",
        "        x = clip_model.token_embedding(text).type(clip_model.dtype)\n",
        "        _, counts = torch.unique((text == token_id).nonzero(as_tuple=True)[0], return_counts=True)\n",
        "        cum_sum = torch.cat((torch.zeros(1, device=torch.device('cuda')).int(), torch.cumsum(counts, dim=0)[:-1]))\n",
        "        first_vstar_indexes = (text == token_id).nonzero()[cum_sum][:, 1]\n",
        "        rep_idx = torch.cat([(first_vstar_indexes + n).unsqueeze(0) for n in range(num_vstar)])\n",
        "\n",
        "        if v_star.shape[0] == x.shape[0]:\n",
        "            if len(v_star.shape) == 2:\n",
        "                v_star = v_star.unsqueeze(1)\n",
        "            x[torch.arange(x.shape[0]).repeat_interleave(num_vstar).reshape(x.shape[0],\n",
        "                                                                            num_vstar), rep_idx.T] = v_star.to(\n",
        "                x.dtype)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = clip_model.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = clip_model.ln_final(x).type(clip_model.dtype)\n",
        "\n",
        "        if pooling:\n",
        "            if proj:\n",
        "                x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ clip_model.text_projection\n",
        "            else:\n",
        "                x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]\n",
        "        return x\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super().__call__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, prompt, image_features):\n",
        "        img_features = self.pre_inversion_map(image_features)\n",
        "\n",
        "        v_star = self.phi(img_features)\n",
        "\n",
        "        if self.phi_proj:\n",
        "            v_star = self.phi_map(v_star)\n",
        "\n",
        "        text_input = prompt\n",
        "\n",
        "        features = self.encode_with_vstar(self.clip_model, text_input, v_star, proj=self.text_proj).float()\n",
        "\n",
        "        if self.post_proj:\n",
        "            features = self.post_inversion_map(features)\n",
        "\n",
        "\n",
        "        return features\n",
        "\n",
        "CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ = 1024\n",
        "\n",
        "\n",
        "class LinearProjection(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers, drop_probs):\n",
        "        super(LinearProjection, self).__init__()\n",
        "\n",
        "        map_layers = [nn.Linear(input_dim, output_dim),\n",
        "                      nn.Dropout(p=drop_probs[0])]\n",
        "\n",
        "        for _ in range(1, num_layers):\n",
        "            map_layers.extend(\n",
        "                [nn.ReLU(), nn.Linear(output_dim, output_dim), nn.Dropout(p=drop_probs[0])])\n",
        "\n",
        "        self.proj = nn.Sequential(*map_layers)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super().__call__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "class HateClassifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = args.dataset\n",
        "        self.num_mapping_layers = args.num_mapping_layers\n",
        "        self.map_dim = args.map_dim\n",
        "        self.fusion = args.fusion\n",
        "        self.num_pre_output_layers = args.num_pre_output_layers\n",
        "        self.lr = args.lr\n",
        "        self.weight_decay = args.weight_decay\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.name = args.name\n",
        "        self.fast_process = args.fast_process\n",
        "\n",
        "        self.proj_map = args.proj_map\n",
        "\n",
        "        self.pretrained_proj = args.pretrained_proj_weights\n",
        "        self.freeze_proj = args.freeze_proj_layers\n",
        "\n",
        "        self.convex_tensor = args.convex_tensor\n",
        "        self.comb_proj = args.comb_proj\n",
        "        self.comb_fusion = args.comb_fusion\n",
        "\n",
        "        self.enh_text = args.enh_text\n",
        "        self.phi_freeze = args.phi_freeze\n",
        "        self.text_inv_proj = args.text_inv_proj\n",
        "        self.phi_inv_proj = args.phi_inv_proj\n",
        "        self.post_inv_proj = args.post_inv_proj\n",
        "\n",
        "        self.acc = torchmetrics.Accuracy(task='binary')\n",
        "        self.auroc = torchmetrics.AUROC(task='binary')\n",
        "\n",
        "        self.pretrained_weights_path = f'./resources/pretrained_weights/{self.dataset}'\n",
        "\n",
        "        self.clip_model, _ = clip.load(\"ViT-L/14\", device=\"cuda\", jit=False)\n",
        "\n",
        "        self.clip_model.visual.proj = None\n",
        "        self.clip_model.float()\n",
        "\n",
        "        for _, p in self.clip_model.named_parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        self.image_map = LinearProjection(CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.map_dim,\n",
        "                                          self.num_mapping_layers, args.drop_probs)\n",
        "        self.text_map = LinearProjection(self.clip_model.token_embedding.embedding_dim, self.map_dim,\n",
        "                                         self.num_mapping_layers, args.drop_probs)\n",
        "\n",
        "        if self.name in ['hate-clipper', 'adaptation']:\n",
        "            if args.fusion == 'align':\n",
        "                pre_output_input_dim = self.map_dim\n",
        "            elif args.fusion == 'concat':\n",
        "                pre_output_input_dim = self.map_dim * 2\n",
        "        elif self.name == 'text-only':\n",
        "            if self.proj_map:\n",
        "                pre_output_input_dim = self.map_dim\n",
        "            else:\n",
        "                pre_output_input_dim = self.clip_model.token_embedding.embedding_dim\n",
        "        elif self.name == 'image-only':\n",
        "            if self.proj_map:\n",
        "                pre_output_input_dim = self.map_dim\n",
        "            else:\n",
        "                pre_output_input_dim = CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ\n",
        "\n",
        "        elif self.name == 'sum':\n",
        "            # proj_map is used by default\n",
        "            pre_output_input_dim = self.map_dim\n",
        "\n",
        "        elif self.name == 'combiner':\n",
        "            # proj_map is used by default\n",
        "            self.comb = Combiner(self.convex_tensor, self.map_dim, self.comb_proj, self.comb_fusion)\n",
        "\n",
        "            if self.pretrained_proj:\n",
        "                # Load pre-trained weights\n",
        "                assert self.num_mapping_layers == 1\n",
        "                if self.dataset == 'hmc':\n",
        "                    assert self.map_dim in [1024, 768]\n",
        "                    weights = f'hmc_{self.map_dim}_projection_embeddings.pt'\n",
        "                elif self.dataset == 'harmeme':\n",
        "                    assert self.map_dim == 768\n",
        "                    weights = f'harmeme_{self.map_dim}_projection_embeddings.pt'\n",
        "                else:\n",
        "                    raise ValueError()\n",
        "\n",
        "                state_dict = torch.load(f'{self.pretrained_weights_path}/{weights}')['state_dict']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    self.image_map.proj[0].weight.copy_(state_dict['image_proj_weight'])\n",
        "                    self.image_map.proj[0].bias.copy_(state_dict['image_proj_bias'])\n",
        "                    self.text_map.proj[0].weight.copy_(state_dict['text_proj_weight'])\n",
        "                    self.text_map.proj[0].bias.copy_(state_dict['text_proj_bias'])\n",
        "\n",
        "                if self.freeze_proj:\n",
        "                    # freeze projection layers\n",
        "                    for name, p in self.image_map.named_parameters():\n",
        "                        p.requires_grad_(False)\n",
        "                    for name, p in self.text_map.named_parameters():\n",
        "                        p.requires_grad_(False)\n",
        "\n",
        "            pre_output_input_dim = self.map_dim\n",
        "\n",
        "        elif self.name == 'text-inv':\n",
        "            self.text_inv = TextualInversion(self.clip_model, CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.phi_inv_proj,\n",
        "                                             self.text_inv_proj, self.post_inv_proj, args.drop_probs, self.phi_freeze,\n",
        "                                             self.enh_text, self.map_dim, self.num_mapping_layers)\n",
        "\n",
        "            pre_output_input_dim = self.text_inv.output_dim\n",
        "\n",
        "        elif self.name == 'text-inv-fusion':\n",
        "            self.text_inv = TextualInversion(self.clip_model, CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.phi_inv_proj,\n",
        "                                             self.text_inv_proj, self.post_inv_proj, args.drop_probs, self.phi_freeze,\n",
        "                                             self.enh_text, self.map_dim, self.num_mapping_layers)\n",
        "\n",
        "            self.image_map = LinearProjection(CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.map_dim,\n",
        "                                              self.num_mapping_layers, args.drop_probs)\n",
        "\n",
        "            pre_output_input_dim = self.text_inv.output_dim\n",
        "\n",
        "        elif self.name == 'text-inv-comb':\n",
        "            self.comb = Combiner(self.convex_tensor, self.map_dim, self.comb_proj, self.comb_fusion)\n",
        "\n",
        "            self.text_inv = TextualInversion(self.clip_model, CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.phi_inv_proj,\n",
        "                                             self.text_inv_proj, self.post_inv_proj, args.drop_probs, self.phi_freeze,\n",
        "                                             self.enh_text, self.map_dim, self.num_mapping_layers)\n",
        "\n",
        "            self.image_map = LinearProjection(CLIP_IMG_ENC_OUTPUT_DIM_BEFORE_PROJ, self.map_dim,\n",
        "                                              self.num_mapping_layers, args.drop_probs)\n",
        "\n",
        "            if self.fusion == 'align':\n",
        "                pre_output_input_dim = self.map_dim\n",
        "            elif self.fusion == 'concat':\n",
        "                pre_output_input_dim = 2 * self.map_dim\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "            if self.pretrained_proj:\n",
        "                assert self.num_mapping_layers == 1\n",
        "\n",
        "                if self.dataset == 'hmc':\n",
        "                    assert self.map_dim in [1024, 768]\n",
        "                    weights = f'hmc_{self.map_dim}_projection_embeddings.pt'\n",
        "                    weights_768 = f'hmc_768_projection_embeddings.pt'\n",
        "\n",
        "                elif self.dataset == 'harmeme':\n",
        "                    assert self.map_dim == 768\n",
        "                    weights_768 = f'harmeme_{self.map_dim}_projection_embeddings.pt'\n",
        "                    weights = weights_768\n",
        "\n",
        "                else:\n",
        "                    raise ValueError()\n",
        "\n",
        "                state_dict = torch.load(f'{self.pretrained_weights_path}/{weights}')['state_dict']\n",
        "                state_dict_768 = torch.load(f'{self.pretrained_weights_path}/{weights_768}')['state_dict']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    self.image_map.proj[0].weight.copy_(state_dict['image_proj_weight'])\n",
        "                    self.image_map.proj[0].bias.copy_(state_dict['image_proj_bias'])\n",
        "                    self.text_inv.pre_inversion_map[0].weight.copy_(state_dict_768['image_proj_weight'])\n",
        "                    self.text_inv.pre_inversion_map[0].bias.copy_(state_dict_768['image_proj_bias'])\n",
        "\n",
        "                if self.freeze_proj:\n",
        "                    # freeze projection layers\n",
        "                    for name, p in self.image_map.proj.named_parameters():\n",
        "                        p.requires_grad_(False)\n",
        "                    for name, p in self.text_inv.pre_inversion_map.named_parameters():\n",
        "                        p.requires_grad_(False)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        pre_output_layers = [nn.Dropout(p=args.drop_probs[1])]\n",
        "        output_input_dim = pre_output_input_dim\n",
        "\n",
        "        if self.num_pre_output_layers >= 1:\n",
        "            pre_output_layers.extend(\n",
        "                [nn.Linear(pre_output_input_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args.drop_probs[2])])\n",
        "            output_input_dim = self.map_dim\n",
        "\n",
        "        for _ in range(1, self.num_pre_output_layers):\n",
        "            pre_output_layers.extend(\n",
        "                [nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args.drop_probs[2])])\n",
        "\n",
        "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
        "        self.output = nn.Linear(output_input_dim, 1)\n",
        "\n",
        "        self.cross_entropy_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    def forward(self, batch):\n",
        "        pass\n",
        "\n",
        "    def compute_CLIP_features_without_proj(self, clip_model, img_input, text_input):\n",
        "\n",
        "        image_features = clip_model.visual(img_input.type(clip_model.dtype))\n",
        "\n",
        "        x = clip_model.token_embedding(text_input).type(clip_model.dtype)\n",
        "        x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = clip_model.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = clip_model.ln_final(x).type(clip_model.dtype)\n",
        "        text_features = x[torch.arange(x.shape[0]), text_input.argmax(dim=-1)]\n",
        "\n",
        "        return image_features, text_features\n",
        "\n",
        "    def common_step(self, batch):\n",
        "        if self.fast_process:\n",
        "            image_features = batch['images']\n",
        "            text_features = batch['texts']\n",
        "        else:\n",
        "            image_features, text_features = self.compute_CLIP_features_without_proj(self.clip_model,\n",
        "                                                                                    batch['pixel_values'],\n",
        "                                                                                    batch['texts'])\n",
        "        if self.enh_text:\n",
        "            prompt = batch['enhanced_texts']\n",
        "        else:\n",
        "            prompt = batch['simple_prompt']\n",
        "\n",
        "        output = {}\n",
        "\n",
        "        if self.name in ['hate-clipper', 'adaptation']:\n",
        "            image_features = self.image_map(image_features)\n",
        "\n",
        "            text_features = self.text_map(text_features)\n",
        "            if self.fusion == 'align':\n",
        "                features = torch.mul(image_features, text_features)\n",
        "            elif self.fusion == 'concat':\n",
        "                features = torch.cat([image_features, text_features], dim=1)\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        elif self.name == 'text-only':\n",
        "            if self.proj_map:\n",
        "                features = self.text_map(text_features)\n",
        "            else:\n",
        "                features = text_features\n",
        "\n",
        "        elif self.name == 'image-only':\n",
        "            if self.proj_map:\n",
        "                features = self.image_map(image_features)\n",
        "            else:\n",
        "                features = image_features\n",
        "\n",
        "        elif self.name == 'sum':\n",
        "            img_features = self.image_map(image_features)\n",
        "            txt_features = self.text_map(text_features)\n",
        "            features = img_features + txt_features\n",
        "\n",
        "        elif self.name == 'combiner':\n",
        "            proj_img_features = self.image_map(image_features)\n",
        "            proj_txt_features = self.text_map(text_features)\n",
        "\n",
        "            features = self.comb(proj_img_features, proj_txt_features)\n",
        "\n",
        "        elif self.name == 'text-inv':\n",
        "            features = self.text_inv(prompt, image_features)\n",
        "\n",
        "        elif self.name == 'text-inv-fusion':\n",
        "            features = self.text_inv(prompt, image_features)\n",
        "\n",
        "            img_projection = self.image_map(image_features)\n",
        "\n",
        "            if self.fusion == 'concat':\n",
        "                features = torch.cat([features, img_projection], dim=1)\n",
        "            elif self.fusion == 'align':\n",
        "                features = torch.mul(features, img_projection)\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        elif self.name == 'text-inv-comb':\n",
        "            txt_features = self.text_inv(prompt, image_features)\n",
        "            print(txt_features.shape)\n",
        "\n",
        "            img_projection = self.image_map(image_features)\n",
        "            print(img_projection)\n",
        "\n",
        "            features = self.comb(img_projection, txt_features)\n",
        "\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        features_pre_output = self.pre_output(features)\n",
        "        logits = self.output(features_pre_output).squeeze(dim=1)\n",
        "        print(logits)\n",
        "        preds_proxy = torch.sigmoid(logits)\n",
        "        print('preds_proxy:', preds_proxy)\n",
        "        preds = (preds_proxy >= 0.5).long()\n",
        "        print(\"preds:\", preds.item())\n",
        "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
        "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
        "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
        "\n",
        "        return output\n",
        "\n",
        "        prefix = prefix_map[dataloader_idx]\n",
        "\n",
        "        output = self.common_step(batch)\n",
        "\n",
        "        self.log(f'{prefix}/accuracy', output['accuracy'])\n",
        "        self.log(f'{prefix}/auroc', output['auroc'])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.acc.reset()\n",
        "        self.auroc.reset()\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.acc.reset()\n",
        "        self.auroc.reset()\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        self.acc.reset()\n",
        "        self.auroc.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        param_dicts = [\n",
        "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def create_model(args):\n",
        "    model = HateClassifier(args=args)\n",
        "    return model"
      ],
      "metadata": {
        "id": "JVTKkG-2xkVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = argparse.Namespace(\n",
        "    dataset='hmc',\n",
        "    image_size=224,\n",
        "    num_mapping_layers=1,\n",
        "    map_dim=1024,\n",
        "    fusion='align',\n",
        "    num_pre_output_layers=1,\n",
        "    drop_probs=[0.2, 0.4, 0.1],\n",
        "    gpus='0',\n",
        "    limit_train_batches=1.0,\n",
        "    limit_val_batches=1.0,\n",
        "    max_steps=-1,\n",
        "    max_epochs=-1,\n",
        "    log_every_n_steps=25,\n",
        "    val_check_interval=1.0,\n",
        "    batch_size=1,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    gradient_clip_val=0.1,\n",
        "    proj_map=False,\n",
        "    pretrained_proj_weights=True,\n",
        "    freeze_proj_layers=True,\n",
        "    comb_proj=True,\n",
        "    comb_fusion='align',\n",
        "    convex_tensor=False,\n",
        "    text_inv_proj=True,\n",
        "    phi_inv_proj=True,\n",
        "    post_inv_proj=True,\n",
        "    enh_text=True,\n",
        "    phi_freeze=True,\n",
        "    name='text-inv-comb',\n",
        "    pretrained_model='hmc_text-inv-comb_best.ckpt',\n",
        "    reproduce=True,\n",
        "    print_model=True,\n",
        "    fast_process=False\n",
        ")\n",
        "\n",
        "model = HateClassifier.load_from_checkpoint(f'resources/pretrained_models/{args.pretrained_model}', args=args,map_location='cuda:0')\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((args.image_size, args.image_size)),\n",
        "    transforms.CenterCrop(args.image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])"
      ],
      "metadata": {
        "id": "VR-Aq995xkaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "image_path = '/content/01235.png'\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image_tensor = transform(image).unsqueeze(0)\n",
        "image_tensor = image_tensor.to('cuda')\n",
        "\n",
        "# ImpaintingResult = transform_image('/content/memetest.jpg')\n",
        "# CaptioningResult = process_meme_caption(image_path)\n",
        "OcrResult = MyOCR(image_path)\n",
        "# CaptioningResult = CaptioningFunc(image_path)\n",
        "\n",
        "batch = {\n",
        "    'pixel_values': image_tensor,\n",
        "    'texts': clip.tokenize('a photo of $ ,', context_length=77).to('cuda'),\n",
        "    'labels': torch.tensor([1]).to('cuda'),\n",
        "    'enhanced_texts': clip.tokenize(f'a picture of $ , {OcrResult}', context_length=77).to('cuda')\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.common_step(batch)"
      ],
      "metadata": {
        "id": "hxC8ygVqbbcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo"
      ],
      "metadata": {
        "id": "JZHbpZ807V9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = \"a better question what do american voters need to do to defeat antifa amp their democrat sponsors\" # @param {type:\"string\"}\n",
        "import magic\n",
        "mime = magic.Magic(mime=True)\n",
        "\n",
        "# Check if the variable is a non-empty string\n",
        "if isinstance(input_data, str):\n",
        "    # Check if the string represents an existing file\n",
        "    if os.path.exists(input_data):\n",
        "        dataType = mime.from_file(input_data)\n",
        "        # print(dataType)\n",
        "        if dataType.startswith(\"image\"):\n",
        "            pred = reader.readtext(input_data, detail = 0)\n",
        "            pred = ' '.join(pred)\n",
        "            # output = perform_inference(input_data)\n",
        "            if pred == \"\":\n",
        "                print(\"Data type: pure image\")\n",
        "                output = perform_inference(input_data)\n",
        "            else:\n",
        "                print(\"Data type: meme\")\n",
        "                start = time.time()\n",
        "                caption = process_meme_caption(input_data)\n",
        "                print(\"meme content:\", pred + '. ' + caption)\n",
        "                output = perform_inference(pred + '. ' + caption, meme=True)\n",
        "        elif dataType.startswith(\"text\"):\n",
        "            print(\"Data type: text\")\n",
        "            with open(input_data, 'r') as text_file:\n",
        "                text_data = text_file.read()\n",
        "            output = perform_inference(text_data)\n",
        "        elif dataType.startswith(\"audio\"):\n",
        "            print(\"Data type: audio\")\n",
        "            text = audio_model.transcribe(input_data)\n",
        "            print(\"audio content:\", text['text'])\n",
        "            output = perform_inference(text['text'])\n",
        "    else:\n",
        "        print(\"Data type: text\")\n",
        "        output = perform_inference(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ULWcpy-rbWD",
        "outputId": "351e3f8d-ff9a-42df-8701-b739f7d2c955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: text\n",
            "Prediction: NonBully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qCHk27QY6yZe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}